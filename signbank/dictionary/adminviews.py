# -*- coding: utf-8 -*-
from __future__ import unicode_literals

import csv
import json
from collections import defaultdict
from uuid import uuid4

import djqscsv
from django.conf import settings
from django.contrib import messages
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.contrib.postgres.aggregates import StringAgg
from django.core.exceptions import PermissionDenied
from django.db.models import Count, F, OuterRef, Prefetch, Q, Sum, Value
from django.db.models.fields import CharField, NullBooleanField
from django.db.models.functions import Concat
from django.http import HttpResponse, JsonResponse
from django.shortcuts import get_object_or_404
from django.template.loader import render_to_string
from django.urls import reverse
from django.utils.translation import get_language
from django.utils.translation import gettext as _
from django.views.generic.detail import DetailView
from django.views.generic.list import ListView
from django_comments.models import Comment
from guardian.shortcuts import (get_objects_for_user, get_perms,
                                get_users_with_perms)
from reversion.models import Version
from tagging.models import Tag, TaggedItem

from ..comments import CommentTagForm
from ..video.forms import GlossVideoForGlossForm
from ..video.models import GlossVideo, GlossVideoToken
from .forms import (GlossRelationForm, GlossRelationSearchForm,
                    GlossSearchForm, MorphologyForm, RelationForm, TagsAddForm)
from .models import (Dataset, FieldChoice, Gloss, GlossRelation,
                     GlossTranslations, GlossURL, Lemma, ManualValidationAggregation, MorphologyDefinition,
                     Relation, RelationToForeignSign, ShareValidationAggregation, Translation,
                     ValidationRecord)


class GlossListView(ListView):
    model = Gloss
    template_name = 'dictionary/admin_gloss_list.html'
    paginate_by = 100

    def get_context_data(self, **kwargs):
        # Call the base implementation first to get a context
        context = super(GlossListView, self).get_context_data(**kwargs)
        # Add in a QuerySet
        context['searchform'] = GlossSearchForm(self.request.GET)
        # Get allowed datasets for user (django-guardian)
        allowed_datasets = get_objects_for_user(
            self.request.user, 'dictionary.view_dataset')
        # Filter the forms dataset field for the datasets user has permission to.
        context['searchform'].fields["dataset"].queryset = Dataset.objects.filter(
            id__in=[x.id for x in allowed_datasets])

        populate_tags_for_object_list(
            context['object_list'], model=self.object_list.model)

        if 'order' not in self.request.GET:
            context['order'] = 'idgloss'
        else:
            context['order'] = self.request.GET.get('order')

        return context

    def get_paginate_by(self, queryset):
        """
        Paginate by specified value in querystring, or use default class property value.
        """
        return self.request.GET.get('paginate_by', self.paginate_by)

    def render_to_response(self, context, **kwargs):

        # Look for a 'format=json' GET argument
        if self.request.GET.get("format") == "CSV-standard":
            return self.subquery_render_to_csv_response(context)
        elif self.request.GET.get("format") == "CSV-ready-for-validation":
            return self.ready_for_validation_render_to_csv_response(context)
        elif self.request.GET.get("format") == "CSV-validation-results":
            return self.validation_results_render_to_csv_response(context)
        else:
            return super(GlossListView, self).render_to_response(context)

    #
    # NOTE to future devs.
    # The reason the code below is prefixed 'subquery' is that, for speed, we use Django
    # annotated subqueries here to push as much of the work onto Postgres as we can.
    #
    # The old code was prefixed 'pythonic', because it worked by analysing the returned results
    # in python code, using nested loops.
    # This was slow, even after optimisation, and on a fast system a full query took over 20 seconds
    # to complete. At writing our Heroku timeout limit is 30 seconds, and the client did in fact get
    # timeouts sometimes, which made us develop this Postgres low-level version.
    #
    # The old 'pythonic' code may be found at this commit:
    # https://github.com/ODNZSL/NZSL-signbank/commit/3ec77f1694350801ce29a21e8e6cfd61cc21523e
    #
    # Some fields were too hard to retrieve via subquery, eg. 'keywords', a deprecated field, which was
    # an amalgamation of all translations. If devs strike a situation in future where they need to
    # retrieve an especially complex field, this will probably have to be implemented 'pythonically'.
    # It is presumed that the speedup provided by the Postgres subquery approach will sufficiently
    # offset any slowdown that results.
    #

    # Field in Signbank -> Field in NZSL Dictionary
    # These are in the order that Micky Vale specified NZSL want them in.
    # Please keep any redundant entries (eg. 'id' -> 'id'), as a reminder of that field ordering.
    # ForeignKey fields on the left need to use double-underscore filter syntax -
    #  eg. strong_handshape__english_name for accessing FieldChoices.
    # The left fields that end in _aggregate are aliases used in annotated subqueries, see below.
    subquery_signbank_field_to_dictionary_field = {
        'id':                               'id',                           # Signbank ID
        'dataset__name':                    'dataset',                      # Dataset
        'variant_no':                       'variant_number',

        # aggregated multi-value fields
        'gloss_main_aggregate':             'gloss_main',                   # Gloss
        'gloss_secondary_aggregate':        'gloss_secondary',
        'gloss_minor_aggregate':            'gloss_minor',

        'gloss_maori_aggregate':            'gloss_maori',                  # Gloss MƒÅori
        'strong_handshape__english_name':   'handshape',
        'location__english_name':           'location_name',
        'one_or_two_hand':                  'one_or_two_handed',

        # aggregated multi-value field
        'wordclasses_aggregate':            'word_classes',

        'inflection_manner_degree':         'inflection_manner_and_degree',
        'inflection_temporal':              'inflection_temporal',
        'inflection_plural':                'inflection_plural',
        'directional':                      'is_directional',
        'locatable':                        'is_locatable',
        'number_incorporated':              'contains_numbers',
        'fingerspelling':                   'is_fingerspelling',
        'videoexample1':                   'videoexample1',
        'videoexample1_translation':       'videoexample1_translation',
        'videoexample2':                   'videoexample2',
        'videoexample2_translation':       'videoexample2_translation',
        'videoexample3':                   'videoexample3',
        'videoexample3_translation':       'videoexample3_translation',
        'videoexample4':                   'videoexample4',
        'videoexample4_translation':       'videoexample4_translation',
        'hint':                             'hint',
        'notes':                            'usage_notes',                  # Notes
        'age_variation__english_name':      'age_groups',
        'relationtoforeignsign__other_lang':'related_to',
        'usage__english_name':              'usage',

        # aggregated multi-value field
        'semantic_field_aggregate':         'semantic_field',

        # The fields below are not necessary but Micky Vale is happy for them to remain
        'dataset__signlanguage__name':      'signlanguage',
        # keywords - in the old system this was an aggregation of all translations.
        # It proved too difficult to reproduce using annotated subqueries.
        #'keywords'                         'keywords'
        'created_at':                       'created_at',
        'created_by__username':             'created_by',
        'updated_at':                       'updated_at',
        'updated_by__username':             'updated_by',
        }

    # string delimeter for aggregated multi-value fields
    CSV_AGG_DELIM = "; "

    def subquery_render_to_csv_response(self, context):
        if not self.request.user.has_perm('dictionary.export_csv'):
            msg = _("You do not have permissions to export to CSV.")
            messages.error(self.request, msg)
            raise PermissionDenied(msg)

        # Create the HttpResponse object with the appropriate CSV header.
        # It is a Python file-like object.
        response = HttpResponse(content_type='text/csv; charset=utf-8')
        response['Content-Disposition'] = 'attachment; filename="dictionary-export.csv"'

        # Set up for outputting CSV.
        writer = csv.writer(response)

        # For speed, we use Django annotated subqueries here to push as much of the work onto Postgres as we can.
        # StringAgg() is a Postgres-specific function exposed in Django.
        # Refs used:
        # https://docs.djangoproject.com/en/4.0/ref/contrib/postgres/aggregates/#stringagg
        # https://docs.djangoproject.com/en/4.0/ref/models/expressions/
        # Some pre-fetching optimizations are already performed by get_queryset().
        csv_queryset = self.get_queryset()\
            .select_related('dataset', 'created_by', 'updated_by', 'strong_handshape', 'location', 'age_variation')\
            .prefetch_related('glosstranslations_set', 'relationtoforeignsign_set',\
                              'usage', 'wordclasses', 'semantic_field')\
            .annotate(
                gloss_main_aggregate=StringAgg(
                    GlossTranslations.objects.filter(
                        gloss=OuterRef('pk'),
                        language__language_code_2char='EN').
                        values('translations')[:1],
                    self.CSV_AGG_DELIM,
                    distinct=True,
                    output_field=CharField()
                ),
                gloss_secondary_aggregate=StringAgg(
                    GlossTranslations.objects.filter(
                        gloss=OuterRef('pk'),
                        language__language_code_2char='EN'
                    ).values('translations_secondary')[:1],
                    self.CSV_AGG_DELIM,
                    distinct=True,
                    output_field=CharField()
                ),
                gloss_minor_aggregate=StringAgg(
                    GlossTranslations.objects.filter(
                        gloss=OuterRef('pk'),
                        language__language_code_2char='EN'
                    ).values('translations_minor')[:1],
                    self.CSV_AGG_DELIM,
                    distinct=True,
                    output_field=CharField()
                ),
                gloss_maori_aggregate=StringAgg(
                    GlossTranslations.objects.filter(
                        gloss=OuterRef('pk'),
                        language__language_code_2char='MI'
                    ).values('translations')[:1],
                    self.CSV_AGG_DELIM,
                    distinct=True,
                    output_field=CharField()
                ),
                semantic_field_aggregate=StringAgg('semantic_field__english_name', self.CSV_AGG_DELIM, distinct=True),
                wordclasses_aggregate=StringAgg('wordclasses__english_name', self.CSV_AGG_DELIM, distinct=True)
            )\
            .values(*self.subquery_signbank_field_to_dictionary_field.keys())

        # Column headers.
        writer.writerow(self.subquery_signbank_field_to_dictionary_field.values())

        # NOTE The following loop is almost exactly what the djqscsv module does internally.
        # We tried using the djqscsv module, with similar annotated subqueries to the above,
        # but discovered we needed more control. Hence we have fallen back on a semi-manual system here.
        # It is still extremely fast.
        for queryset_values_record in csv_queryset:
            arr = []
            items_dict = dict(queryset_values_record.items())
            # force ordering
            for field_name in self.subquery_signbank_field_to_dictionary_field.keys():
                try:
                    items_field = items_dict[field_name]
                    if items_field == None:
                        items_field = ''
                    arr.append(str(items_field))
                except:
                    arr.append('')
            writer.writerow(arr)

        return response

    def ready_for_validation_render_to_csv_response(self, context):
        if not self.request.user.has_perm("dictionary.export_csv"):
            msg = _("You do not have permissions to export to CSV.")
            messages.error(self.request, msg)
            raise PermissionDenied(msg)

        # Create the HttpResponse object with the appropriate CSV header.
        # It is a Python file-like object.
        response = HttpResponse(content_type="text/csv; charset=utf-8")
        response[
            "Content-Disposition"
        ] = 'attachment; filename="ready-for-validation-export.csv"'

        # Set up for outputting CSV.
        writer = csv.writer(response)

        # The queryset may or may not already be filtered for the ready for validation tag.
        # We have to make sure it is filtered by the tag, so we are filtering again
        ready_for_validation_qs = TaggedItem.objects.get_by_model(
            self.get_queryset(),
            settings.TAG_READY_FOR_VALIDATION
        )

        # three types of GlossVideos are imported for validation: illustrations, usage examples
        # and videos.
        # Only the videos have title Main and validation video-type
        gloss_video_qs = GlossVideo.objects.select_related("video_type").filter(
            video_type__isnull=False,
            video_type__field="video_type",
            video_type__english_name="validation",
            videofile__isnull=False,
        )

        csv_queryset = (
            ready_for_validation_qs
            .prefetch_related(
                "glosstranslations_set",
                Prefetch(
                    "glossvideo_set", queryset=gloss_video_qs, to_attr="validation_videos"
                )
            )
            .annotate(
                gloss_main_aggregate=StringAgg(
                    GlossTranslations.objects.filter(
                        gloss=OuterRef("pk"),
                        language__language_code_2char="EN"
                    ).values("translations")[:1],
                    self.CSV_AGG_DELIM,
                    distinct=True,
                    output_field=CharField()
                ),
            )
        )

        headers = [
            "idgloss",
            "gloss_main",
            "video_url"
        ]
        writer.writerow(headers)

        glossvideo_tokens = []

        for gloss_record in csv_queryset:
            row = [gloss_record.idgloss, gloss_record.gloss_main_aggregate]
            # In theory there should only be one video matching the above query, or none.
            if video := next((v for v in gloss_record.validation_videos if v.is_video), None):
                token = uuid4()
                glossvideo_tokens.append(GlossVideoToken(token=token, video=video))
                url = reverse(
                    "video:get_signed_glossvideo_url",
                    kwargs={"token": token, "videoid": video.pk}
                )
                row.append(self.request.build_absolute_uri(url))
            else:
                row.append("")
                row.append("")
            writer.writerow(row)

        GlossVideoToken.objects.bulk_create(glossvideo_tokens)

        return response

    def validation_results_render_to_csv_response(self, context):
        if not self.request.user.has_perm("dictionary.export_csv"):
            msg = _("You do not have permissions to export to CSV.")
            messages.error(self.request, msg)
            raise PermissionDenied(msg)

        # Create the HttpResponse object with the appropriate CSV header.
        # It is a Python file-like object.
        response = HttpResponse(content_type="text/csv; charset=utf-8")
        response[
            "Content-Disposition"
        ] = 'attachment; filename="validation-results-export.csv"'

        # Set up for outputting CSV.
        writer = csv.writer(response)

        # The queryset may or may not already be filtered for the validation:check-results tag.
        # We have to make sure it is filtered by the tag, so we are filtering again
        check_results_qs = TaggedItem.objects.get_by_model(
            self.get_queryset(),
            settings.TAG_VALIDATION_CHECK_RESULTS
        )

        sign_seen_yes_validation_records = ValidationRecord.objects.filter(
            sign_seen=ValidationRecord.SignSeenChoices.YES
        )
        sign_seen_no_validation_records = ValidationRecord.objects.filter(
            sign_seen=ValidationRecord.SignSeenChoices.NO
        )
        sign_seen_not_sure_validation_records = ValidationRecord.objects.filter(
            sign_seen=ValidationRecord.SignSeenChoices.NOT_SURE
        )

        csv_queryset = (
            check_results_qs
            .prefetch_related(
                Prefetch(
                    "validation_records", queryset=sign_seen_yes_validation_records,
                    to_attr="yes_vrs"
                ),
                Prefetch(
                    "validation_records", queryset=sign_seen_no_validation_records,
                    to_attr="no_vrs"
                ), Prefetch(
                    "validation_records", queryset=sign_seen_not_sure_validation_records,
                    to_attr="not_sure_vrs"
                ),
                Prefetch("share_validation_aggregations", to_attr="share_vas"),
                Prefetch("manual_validation_aggregation", to_attr="manual_vas")
            )
        )
        gloss_pks = csv_queryset.values_list("pk", flat=True)
        gloss_share_comment_map = {pk: [] for pk in gloss_pks}
        share_comments = Comment.objects.filter(
            content_type=ContentType.objects.get_for_model(Gloss),
            object_pk__in=[str(pk) for pk in gloss_pks],
            is_public=False
        )
        for comment in share_comments:
            gloss_share_comment_map[int(comment.object_pk)].append(comment)

        headers = [
            "idgloss",
            "have seen sign - yes",
            "have seen sign - no",
            "have seen sign - not sure",
            "total",
            "comments"
        ]
        writer.writerow(headers)

        for gloss_record in csv_queryset:
            sign_seen_yes = sum([
                len(gloss_record.yes_vrs),
                sum([share_va.agrees for share_va in gloss_record.share_vas]),
                sum([manual_va.sign_seen_yes for manual_va in gloss_record.manual_vas])
            ])
            sign_seen_no = sum([
                len(gloss_record.no_vrs),
                sum([share_va.disagrees for share_va in gloss_record.share_vas]),
                sum([manual_va.sign_seen_no for manual_va in gloss_record.manual_vas])
            ])
            sign_seen_not_sure = sum([
                len(gloss_record.not_sure_vrs),
                sum([manual_va.sign_seen_not_sure for manual_va in gloss_record.manual_vas])
            ])
            total = sum([sign_seen_yes, sign_seen_no, sign_seen_not_sure])
            records_with_comments = gloss_record.yes_vrs + gloss_record.no_vrs + gloss_record.not_sure_vrs
            records_with_comments = [x for x in records_with_comments if x.comment != ""]
            comment = ""
            for record in records_with_comments:
                comment += f"{record.respondent_first_name} {record.respondent_last_name}: {record.comment} | "
            for share_comment in gloss_share_comment_map[gloss_record.pk]:
                comment += f"{share_comment.user_name}: {share_comment.comment} | "
            for manual_va in gloss_record.manual_vas:
                if manual_va.comments:
                    comment += f"{manual_va.group}: {manual_va.comments} | "
            row = [
                gloss_record.idgloss,
                sign_seen_yes,
                sign_seen_no,
                sign_seen_not_sure,
                total,
                comment
            ]
            writer.writerow(row)

        return response

    def get_queryset(self):
        # get query terms from self.request
        qs = Gloss.objects.all()

        # Filter in only objects in the datasets the user has permissions to.
        allowed_datasets = get_objects_for_user(
            self.request.user, 'dictionary.view_dataset')
        qs = qs.filter(dataset__in=allowed_datasets)

        get = self.request.GET

        # Search for multiple datasets (if provided)
        vals = get.getlist('dataset', [])
        if vals != []:
            qs = qs.filter(dataset__in=vals)

        if 'search' in get and get['search'] != '':
            val = get['search']
            # Searches for multiple fields at the same time. Looking if any of the fields match.
            query = (Q(idgloss__icontains=val) | Q(idgloss_mi__icontains=val) | Q(notes__icontains=val) |
                     Q(translation__keyword__text__icontains=val))
            qs = qs.filter(query)

        if 'gloss' in get and get['gloss'] != '':
            val = get['gloss']
            # Search for glosses starting with a string, case sensitive
            query = Q(idgloss__istartswith=val)
            qs = qs.filter(query)

        if 'idgloss_mi' in get and get['idgloss_mi'] != '':
            val = get['idgloss_mi']
            qs = qs.filter(idgloss_mi__istartswith=val)
        if 'keyword' in get and get['keyword'] != '':
            if 'trans_lang' in get and get['trans_lang'] != '':
                val = get['keyword']
                lang = get['trans_lang']
                qs = qs.filter(
                    translation__keyword__text__icontains=val, translation__language__in=lang)

        if 'published' in get and get['published'] != '':
            val = get['published'] == 'on'
            qs = qs.filter(published=val)

        # If get has both keys hasvideo and hasnovideo, don't use them to query.
        if not ('hasvideo' in get and 'hasnovideo' in get):
            if 'hasvideo' in get and get['hasvideo'] != '':
                val = get['hasvideo'] != 'on'
                qs = qs.filter(glossvideo__isnull=val)

            if 'hasnovideo' in get and get['hasnovideo'] != '':
                val = get['hasnovideo'] == 'on'
                qs = qs.filter(glossvideo__isnull=val)

        # If gloss has multiple GlossVideos
        if 'multiplevideos' in get and get['multiplevideos'] != '' and get['multiplevideos'] == 'on':
            # Include glosses that have more than one GlossVideo
            qs = qs.annotate(videocount=Count('glossvideo')
                             ).filter(videocount__gt=1)

        # A list of phonology fieldnames
        fieldnames = ['handedness', 'strong_handshape', 'weak_handshape', 'location', 'relation_between_articulators',
                      'absolute_orientation_palm', 'absolute_orientation_fingers', 'relative_orientation_movement',
                      'relative_orientation_location', 'orientation_change', 'handshape_change', 'repeated_movement',
                      'alternating_movement', 'movement_shape', 'movement_direction', 'movement_manner',
                      'contact_type', 'phonology_other', 'mouth_gesture', 'mouthing', 'phonetic_variation',
                      'iconic_image', 'named_entity', 'number_of_occurences', 'fingerspelling',
                      'one_or_two_hand', 'number_incorporated', 'locatable', 'directional', 'variant_no']

        # Language and basic property filters
        vals = get.getlist('dialect', [])
        if vals != []:
            qs = qs.filter(dialect__in=vals)

        vals = get.getlist('language', [])
        if vals != []:
            # Get languages from dataset
            qs = qs.filter(dataset__language__in=vals)

        if 'notes' in get and get['notes'] != '':
            qs = qs.filter(notes__icontains=get['notes'])

        if 'semantic_field' in get and get['semantic_field'] != '':
            vals = get.getlist('semantic_field')
            qs = qs.filter(semantic_field__id__in=vals)

        # phonology and semantics field filters
        for fieldname in fieldnames:
            if fieldname in get:
                key = fieldname + '__exact'
                val = get[fieldname]
                if isinstance(Gloss._meta.get_field(fieldname), NullBooleanField):
                    val = {'0': '', '1': None, '2': True, '3': False}[val]

        if 'tags' in get and get['tags'] != '':
            vals = get.getlist('tags')

            tags = []
            for t in vals:
                tags.extend(Tag.objects.filter(pk=t))

            # search is an implicit AND so intersection
            tqs = TaggedItem.objects.get_intersection_by_model(Gloss, tags)

            # intersection
            qs = qs & tqs

            # print "J :", len(qs)

        qs = qs.distinct()

        if 'nottags' in get and get['nottags'] != '':
            vals = get.getlist('nottags')

            # print "NOT TAGS: ", vals

            tags = []
            for t in vals:
                tags.extend(Tag.objects.filter(name=t))

            # search is an implicit AND so intersection
            tqs = TaggedItem.objects.get_intersection_by_model(Gloss, tags)

            # print "NOT", tags, len(tqs)
            # exclude all of tqs from qs
            qs = [q for q in qs if q not in tqs]

            # print "K :", len(qs)

        if 'relation_to_foreign_signs' in get and get['relation_to_foreign_signs'] != '':
            val = get['relation_to_foreign_signs']
            gloss_ids = RelationToForeignSign.objects.filter(
                other_lang=val).values_list('gloss_id', flat=True)
            qs = qs.filter(id__in=gloss_ids)

        if 'location' in get and get['location'] != '':
            val = get['location']
            qs = qs.filter(location=val)

        if 'one_or_two_handed' in get and get['one_or_two_handed'] != '':
            val = get['one_or_two_handed'] == 'on'
            qs = qs.filter(one_or_two_hand=val)

        if 'example_search' in get and get['example_search'] != '':
            """
                This search is intended to search for gloss IDs in fields videoexample1 to videoexample4. In these
                fields, gloss IDs are within square brackets (eg: cat[123], 123 is the gloss ID).
                When we search for a gloss ID (eg: 123), the search should return all the glosses that contain gloss ID
                123 in one of their videoexample fields.
                Search parameter is just the gloss id, so we are adding []s before doing the search.
            """

            val = get['example_search']
            val = '[' + val + ']'

            query = (Q(videoexample1__icontains=val) | Q(videoexample2__icontains=val) | Q(videoexample3__icontains=val) |
                     Q(videoexample4__icontains=val))
            qs = qs.filter(query)

        if 'age_variation' in get and get['age_variation'] != '':
            val = get['age_variation']
            qs = qs.filter(age_variation=val)

        if 'handedness' in get and get['handedness'] != '':
            val = get['handedness']
            qs = qs.filter(handedness=val)

        if 'strong_handshape' in get and get['strong_handshape'] != '':
            val = get['strong_handshape']
            qs = qs.filter(strong_handshape=val)

        if 'word_classes' in get and get['word_classes'] != '':
            vals = get.getlist('word_classes')
            qs = qs.filter(wordclasses__id__in=vals)

        if 'number_incorporated' in get and get['number_incorporated'] != '':
            val = get['number_incorporated'] == 'on'
            qs = qs.filter(number_incorporated=val)

        if 'locatable' in get and get['locatable'] != '':
            val = get['locatable'] == 'on'
            qs = qs.filter(locatable=val)

        if 'directional' in get and get['directional'] != '':
            val = get['directional'] == 'on'
            qs = qs.filter(directional=val)

        if 'fingerspelling' in get and get['fingerspelling'] != '':
            val = get['fingerspelling'] == 'on'
            qs = qs.filter(fingerspelling=val)

        if 'inflection_temporal' in get and get['inflection_temporal'] != '':
            val = get['inflection_temporal'] == 'on'
            qs = qs.filter(inflection_temporal=val)

        if 'inflection_manner_degree' in get and get['inflection_manner_degree'] != '':
            val = get['inflection_manner_degree'] == 'on'
            qs = qs.filter(inflection_manner_degree=val)

        if 'inflection_plural' in get and get['inflection_plural'] != '':
            val = get['inflection_plural'] == 'on'
            qs = qs.filter(inflection_plural=val)

        if 'relation' in get and get['relation'] != '':
            potential_targets = Gloss.objects.filter(
                idgloss__icontains=get['relation'])
            relations = Relation.objects.filter(target__in=potential_targets)
            potential_pks = [relation.source.pk for relation in relations]
            qs = qs.filter(pk__in=potential_pks)

        if 'hasRelation' in get and get['hasRelation'] != '':

            # Find all relations with this role
            if get['hasRelation'] == 'all':
                relations_with_this_role = Relation.objects.all()
            else:
                relations_with_this_role = Relation.objects.filter(
                    role__exact=get['hasRelation'])

            # Remember the pk of all glosses that take part in the collected
            # relations
            pks_for_glosses_with_correct_relation = [
                relation.source.pk for relation in relations_with_this_role]
            qs = qs.filter(pk__in=pks_for_glosses_with_correct_relation)

        if 'morpheme' in get and get['morpheme'] != '':
            potential_morphemes = Gloss.objects.filter(
                idgloss__icontains=get['morpheme'])
            potential_morphdefs = MorphologyDefinition.objects.filter(
                morpheme__in=[morpheme.pk for morpheme in potential_morphemes])
            potential_pks = [
                morphdef.parent_gloss.pk for morphdef in potential_morphdefs]
            qs = qs.filter(pk__in=potential_pks)

        if 'hasMorphemeOfType' in get and get['hasMorphemeOfType'] != '':
            morphdefs_with_correct_role = MorphologyDefinition.objects.filter(
                role__exact=get['hasMorphemeOfType'])
            pks_for_glosses_with_morphdefs_with_correct_role = [
                morphdef.parent_gloss.pk for morphdef in morphdefs_with_correct_role]
            qs = qs.filter(
                pk__in=pks_for_glosses_with_morphdefs_with_correct_role)

        # Filter by usage
        if 'usage' in get and get['usage'] != '':
            vals = get.getlist('usage')
            qs = qs.filter(usage__id__in=vals)

        # Set order according to GET field 'order'
        if 'order' in get:
            qs = qs.order_by(get['order'])
        else:
            qs = qs.order_by('idgloss')

        # Prefetching translation and dataset objects for glosses to minimize the amount of database queries.
        qs = qs.prefetch_related(Prefetch('translation_set', queryset=Translation.objects.filter(
            language__language_code_2char__iexact=get_language()).select_related('keyword')),
            Prefetch('dataset'),
            # Ordering by version to get the first versions posterfile.
            Prefetch('glossvideo_set', queryset=GlossVideo.objects.all().order_by('version')))

        # Saving querysets results to sessions, these results can then be used elsewhere (like in gloss_detail)
        # Flush the previous queryset (just in case)
        self.request.session['search_results'] = None

        # Check if QuerySet has filters applied (user is searching for something)
        # TODO: Future me or future other developer, it could be useful to find a better way to do this, if one exists
        if hasattr(qs.query.where, 'children') and len(qs.query.where.children) > 1:
            # This comparison has been changed from >0 to >1 due to checking for WHERE dataset.id IN.
            items = []
            for item in qs:
                items.append(dict(id=item.id, gloss=item.idgloss))

            self.request.session['search_results'] = items

        return qs


def populate_tags_for_object_list(object_list, model):
    """Inserts tags for each item in a list of objects."""
    # Get the ContentType for the model.
    content_type = ContentType.objects.get_for_model(model)
    # Get TaggedItems of selected ContentType for all the list items that have TaggedItems.
    tagged_items = TaggedItem.objects.filter(content_type=content_type,
                                             object_id__in=[obj.pk for obj in object_list])
    tagged_items = tagged_items.select_related('tag')
    tags_map = defaultdict(list)
    for tagged_item in tagged_items:
        tags_map[tagged_item.object_id].append(tagged_item.tag)
        for obj in object_list:
            obj.cached_tags = tags_map[obj.pk]


def populate_tags_for_queryset(queryset):
    """Inserts tags for each item in queryset."""
    # Get the ContentType for the queryset objects.
    content_type = ContentType.objects.get_for_model(queryset.model)
    # Get TaggedItems of selected ContentType for all the queryset items that have TaggedItems.
    tagged_items = TaggedItem.objects.filter(content_type=content_type,
                                             object_id__in=queryset.values_list('pk', flat=True))
    tagged_items = tagged_items.select_related('tag')
    tags_map = defaultdict(list)
    for tagged_item in tagged_items:
        tags_map[tagged_item.object_id].append(tagged_item.tag)
        for obj in queryset:
            obj.cached_tags = tags_map[obj.pk]


class GlossDetailView(DetailView):
    model = Gloss
    context_object_name = 'gloss'

    def dispatch(self, request, *args, **kwargs):
        obj = self.get_object()
        # Check that the user has object level permission (django-guardian) to this objects dataset object.
        if 'view_dataset' not in get_perms(request.user, obj.dataset):
            msg = _("You do not have permissions to view glosses of this dataset.")
            messages.error(request, msg)
            raise PermissionDenied(msg)
        return super(GlossDetailView, self).dispatch(request, *args, **kwargs)

    def get_context_data(self, **kwargs):
        # Call the base implementation first to get a context
        context = super(GlossDetailView, self).get_context_data(**kwargs)
        gloss = context['gloss']
        dataset = gloss.dataset
        context['dataset'] = dataset
        context['dataset_users'] = list(get_users_with_perms(
            dataset).values_list('username', flat=True))
        context['tagsaddform'] = TagsAddForm()
        context['commenttagform'] = CommentTagForm()
        context['glossvideoform'] = GlossVideoForGlossForm()
        context['field_choices'] = Gloss.get_choice_lists()

        context['assignable_users'] = list(
            User.objects
                .filter(is_staff=True, is_active=True)
                .annotate(full_name=Concat(F('first_name'), Value(' '), F('last_name'), output_field=CharField()))
                .values(label=F('full_name'), value=F('id')))

        context['lemmas'] = list(Lemma.objects.values('name'))
        context['relationform'] = RelationForm()
        context['morphologyform'] = MorphologyForm()
        context['glossrelationform'] = GlossRelationForm(
            initial={'source': gloss.id, })
        # Choices for GlossRelationForm
        context['dataset_glosses'] = json.dumps(list(Gloss.objects.filter(
            dataset=dataset).values(label=F('idgloss'), value=F('id'))))
        # GlossRelations for this gloss
        context['glossrelations'] = GlossRelation.objects.filter(source=gloss)
        context['glossrelations_reverse'] = GlossRelation.objects.filter(
            target=gloss)
        context['glossurls'] = GlossURL.objects.filter(gloss=gloss)
        context['translation_languages_and_translations'] = gloss.get_translations_for_translation_languages()

        if self.request.user.is_staff:

            # Using the Reversion middleware, get version history data:
            # This is an iterable of Version instance, each of which contains data including:
            #   * A dict of fieldname => field for this model (in this case Gloss)
            #   * A Revision instance with:
            #       Metadata stored when the Version was created
            #       References to the state of all other model instances under Reversion control at the time the Version was saved
            #           (eg. Translation instances)
            version_history = Version.objects.get_for_object(
                gloss).prefetch_related('revision__user')

            # Get ContentType for Translation model so we can retrieve Translation instance states from Reversion
            translation_contenttype = ContentType.objects.get_for_model(
                Translation)

            revisions = []
            revisions_ignore = ('updated_at')
            gloss_fieldchoice_fields = ['handedness', 'strong_handshape', 'weak_handshape', 'location', 'relation_between_articulators',
                                        'absolute_orientation_palm', 'absolute_orientation_fingers', 'relative_orientation_movement',
                                        'relative_orientation_location', 'orientation_change', 'handshape_change', 'movement_shape',
                                        'movement_direction', 'movement_manner', 'contact_type', 'wordclasses', 'usage', 'named_entity',
                                        'semantic_field', 'signer', 'age_variation']

            # Determine which fields have changed
            for i, version_hist_entry in enumerate(version_history):
                if i+1 >= len(version_history):
                    continue

                # Regular fields
                for key, value in version_hist_entry.field_dict.items():
                    if key in revisions_ignore:
                        continue

                    changed_new = value
                    changed_old = version_history[i+1].field_dict[key]

                    if changed_new != changed_old:

                        # Reversion gives us the field name, which for a ForeignKey is sometimes the db column name with '_id'
                        # NOTE removesuffix() is new in Python 3.9
                        keycut = key.removesuffix('_id')

                        # Use the raw field name (key) to retrieve the field verbose name
                        # This works in Django regardless of whether the key is a field name or db column name
                        changed_field_verbosename = Gloss._meta.get_field(
                            key).verbose_name

                        if keycut in gloss_fieldchoice_fields:

                            # If we were given a straight FieldChoice field name, then the values contained in the revision
                            # are primary keys of FieldChoice records.
                            # But if we were given a dictionary_fieldchoice db column name - ie. if it had '_id' on the end - then
                            # the value in the revision is a (single, unique) machine_value number instead.
                            # We have to adjust our QuerySet filters accordingly.
                            if key != keycut:
                                changed_old_qs = FieldChoice.objects.filter(
                                    machine_value=changed_old).values_list('english_name', flat=True)
                                changed_new_qs = FieldChoice.objects.filter(
                                    machine_value=changed_new).values_list('english_name', flat=True)
                            else:
                                changed_old_qs = FieldChoice.objects.filter(
                                    pk__in=changed_old).values_list('english_name', flat=True)
                                changed_new_qs = FieldChoice.objects.filter(
                                    pk__in=changed_new).values_list('english_name', flat=True)
                            changed_old = ', '.join(list(changed_old_qs))
                            changed_new = ', '.join(list(changed_new_qs))

                        if changed_old in ('', None):
                            changed_old = '-'
                        if changed_new in ('', None):
                            changed_new = '-'

                        revisions.append((version_hist_entry.revision.user.username, version_hist_entry.revision.date_created,
                                          changed_field_verbosename, changed_old, changed_new))

                # Translations
                translations_new = list(version_history[i].revision.version_set.filter(
                    content_type=translation_contenttype).values_list('object_repr', flat=True))
                translations_old = list(version_history[i+1].revision.version_set.filter(
                    content_type=translation_contenttype).values_list('object_repr', flat=True))

                # Ignore pairs of empty lists
                if not (translations_new or translations_old):
                    continue

                # This will catch re-orderings as well
                translations_new_str = ", ".join(translations_new)
                translations_old_str = ", ".join(translations_old)
                if (translations_new_str != translations_old_str):
                    revisions.append((version_hist_entry.revision.user.username, version_hist_entry.revision.date_created,
                                      'Translations', translations_old_str, translations_new_str))

            context['revisions'] = revisions

            gloss_content_type = ContentType.objects.get_for_model(Gloss)
            share_comments = Comment.objects.filter(content_type=gloss_content_type, object_pk=self.object.pk, is_public=False)

            validation_records = ValidationRecord.objects.filter(gloss=self.object)
            validation_records_exist = validation_records.exists()
            validation_record_totals = {
                "sign_seen_yes": validation_records.filter(
                    sign_seen=ValidationRecord.SignSeenChoices.YES).count(),

                "sign_seen_no": validation_records.filter(
                    sign_seen=ValidationRecord.SignSeenChoices.NO).count(),

                "sign_seen_not_sure": validation_records.filter(
                    sign_seen=ValidationRecord.SignSeenChoices.NOT_SURE).count()
            }
            validation_record_totals["totals"] = sum([
                validation_record_totals["sign_seen_yes"],
                validation_record_totals["sign_seen_no"],
                validation_record_totals["sign_seen_not_sure"]
            ])

            share_validation_aggregations = ShareValidationAggregation.objects.filter(
                gloss=self.object)
            share_validations_exist = share_validation_aggregations.exists()
            share_validation_totals = {"agrees": 0, "disagrees": 0, "totals":0}
            for share_validation in share_validation_aggregations:
                share_validation_totals["agrees"] += share_validation.agrees
                share_validation_totals["disagrees"] += share_validation.disagrees
            share_validation_totals["totals"] = sum([
                share_validation_totals["agrees"],
                share_validation_totals["disagrees"]
            ])

            manual_validation_aggregations = ManualValidationAggregation.objects.filter(
                gloss=self.object
            )
            manual_validations_exist = manual_validation_aggregations.exists()
            manual_validations_totals = {
                "sign_seen_yes": 0, "sign_seen_no": 0, "sign_seen_not_sure": 0, "totals": 0
            }
            for manual_validation in manual_validation_aggregations:
                manual_validations_totals["sign_seen_yes"] += manual_validation.sign_seen_yes
                manual_validations_totals["sign_seen_no"] += manual_validation.sign_seen_no
                manual_validations_totals["sign_seen_not_sure"] += manual_validation.sign_seen_not_sure
            manual_validations_totals["totals"] = sum([
                manual_validations_totals["sign_seen_yes"],
                manual_validations_totals["sign_seen_no"],
                manual_validations_totals["sign_seen_not_sure"]
            ])

            totals = {
                "sign_seen_yes": sum([
                    validation_record_totals["sign_seen_yes"],
                    share_validation_totals["agrees"],
                    manual_validations_totals["sign_seen_yes"]
                ]),
                "sign_seen_no": sum([
                    validation_record_totals["sign_seen_no"],
                    share_validation_totals["disagrees"],
                    manual_validations_totals["sign_seen_no"]
                ]),
                "sign_seen_not_sure": sum([
                    validation_record_totals["sign_seen_not_sure"],
                    manual_validations_totals["sign_seen_not_sure"]
                ]),
                "overall": sum([
                    validation_record_totals["totals"],
                    share_validation_totals["totals"],
                    manual_validations_totals["totals"]
                ])
            }
            context['share_comments'] = share_comments
            context['validation_records'] = validation_records
            context['share_validations'] = share_validation_aggregations
            context['manual_validations'] = manual_validation_aggregations
            num_totals = [validation_records_exist, share_validations_exist, manual_validations_exist].count(True)
            context['show_totals_row'] = num_totals > 1

            context['validation_record_totals'] = validation_record_totals
            context['share_validation_totals'] = share_validation_totals
            context['manual_validations_totals'] = manual_validations_totals
            context['totals'] = totals

        # Pass info about which fields we want to see
        gl = context['gloss']
        labels = gl.field_labels()

        fields = dict()

        fields['morphology'] = ['number_incorporated', 'locatable', 'directional', 'fingerspelling',
                                'inflection_temporal', 'inflection_manner_degree', 'inflection_plural']
        fields['phonology'] = ['handedness', 'strong_handshape', 'weak_handshape', 'handshape_change',
                               'relation_between_articulators', 'location', 'contact_type',
                               'repeated_movement', 'alternating_movement',
                               'mouth_gesture', 'mouthing', 'phonetic_variation', 'one_or_two_hand']

        fields['semantics'] = ['iconic_image',
                               'named_entity']
        fields['usage'] = ['variant_no']
        fields['examples'] = ['videoexample1', 'videoexample1_translation', 'videoexample2', 'videoexample2_translation',
                              'videoexample3', 'videoexample3_translation', 'videoexample4', 'videoexample4_translation']

        fields['frequency'] = ['number_of_occurences']

        for topic in ['morphology', 'phonology', 'semantics', 'frequency', 'examples', 'usage']:
            context[topic + '_fields'] = []

            for field in fields[topic]:

                try:
                    value = getattr(gl, 'get_' + field + '_display')
                except AttributeError:
                    value = getattr(gl, field)

                if field in ['phonology_other', 'mouth_gesture', 'mouthing', 'phonetic_variation', 'iconic_image']:
                    kind = 'text'
                elif field in ['repeated_movement', 'alternating_movement', 'fingerspelling', 'one_or_two_hand', 'number_incorporated', 'locatable', 'directional', 'inflection_temporal', 'inflection_manner_degree', 'inflection_plural']:
                    kind = 'check'
                else:
                    kind = 'list'

                context[topic +
                        '_fields'].append([value, field, labels[field], kind])

        return context


def gloss_ajax_search_results(request):
    """Returns a JSON list of glosses that match the previous search stored in sessions"""
    if 'search_results' in request.session and request.session['search_results'] != '':
        return JsonResponse(request.session['search_results'], safe=False)
    else:
        return HttpResponse("OK", status=200)


def gloss_ajax_complete(request, prefix):
    """Return a list of glosses matching the search term as a JSON structure suitable for typeahead."""

    query = Q(idgloss__istartswith=prefix)
    qs = Gloss.objects.filter(query)

    result = []

    for g in qs:
        result.append(
            {'idgloss': g.idgloss, 'pk': "%s (%s)" % (g.idgloss, g.pk)})

    return HttpResponse(json.dumps(result), {'content-type': 'application/json'})


def gloss_list_xml(self, dataset_id):
    """Returns all entries in dictionarys idgloss fields in XML form that is supported by ELAN"""
    # http://www.mpi.nl/tools/elan/EAFv2.8.xsd
    dataset = get_object_or_404(Dataset, id=dataset_id)
    return serialize_glosses(dataset,
                             Gloss.objects.filter(
                                 dataset=dataset, exclude_from_ecv=False)
                             .prefetch_related(
                                 Prefetch('translation_set', queryset=Translation.objects.filter(gloss__dataset=dataset)
                                          .select_related('keyword', 'language')),
                                 Prefetch('glosstranslations_set', queryset=GlossTranslations.objects.
                                          filter(gloss__dataset=dataset).select_related('language'))))


def serialize_glosses(dataset, queryset):
    for gloss in queryset:
        # Get Finnish translation equivalents from glosstranslations or from translation_set
        if gloss.glosstranslations_set.exists() and "fin" in [x.language.language_code_3char for x in
                                                              gloss.glosstranslations_set.all()]:
            gloss.trans_fin = [x for x in gloss.glosstranslations_set.all(
            ) if x.language.language_code_3char == "fin"]
        else:
            gloss.trans_fin = [x.keyword.text for x in gloss.translation_set.all() if
                               x.language.language_code_3char == "fin"]
        # Get English translation equivalents from glosstranslations or from translation_set
        if gloss.glosstranslations_set.exists() and "eng" in [x.language.language_code_3char for x in
                                                              gloss.glosstranslations_set.all()]:
            gloss.trans_eng = [x for x in gloss.glosstranslations_set.all() if
                               x.language.language_code_3char == "eng"]
        else:
            gloss.trans_eng = [x.keyword.text for x in gloss.translation_set.all() if
                               x.language.language_code_3char == "eng"]

    xml = render_to_string('dictionary/xml_glosslist_template.xml',
                           {'queryset': queryset, 'dataset': dataset})
    return HttpResponse(xml, content_type="text/xml")


def gloss_list_csv(self, dataset_id):
    """Returns glosses and associated data as a CSV file"""
    dataset = get_object_or_404(Dataset, id=dataset_id)
    return serialize_glosses_csv(dataset,
                                 Gloss.objects.filter(
                                     dataset=dataset, exclude_from_ecv=False)
                                 .prefetch_related(
                                     Prefetch('translation_set', queryset=Translation.objects.filter(gloss__dataset=dataset)
                                              .select_related('keyword', 'language')),
                                     Prefetch('glosstranslations_set', queryset=GlossTranslations.objects.
                                              filter(gloss__dataset=dataset).select_related('language'))))


def serialize_glosses_csv(dataset, queryset):
    return djqscsv.render_to_csv_response(queryset)


class GlossRelationListView(ListView):
    model = GlossRelation
    template_name = 'dictionary/admin_glossrelation_list.html'
    paginate_by = 100

    def get_context_data(self, **kwargs):
        # Call the base implementation first to get a context
        context = super(GlossRelationListView, self).get_context_data(**kwargs)
        context['searchform'] = GlossRelationSearchForm(self.request.GET)
        # Get allowed datasets for user (django-guardian)
        allowed_datasets = get_objects_for_user(
            self.request.user, 'dictionary.view_dataset')
        # Filter the forms dataset field for the datasets user has permission to.
        context['searchform'].fields["dataset"].queryset = Dataset.objects.filter(
            id__in=[x.id for x in allowed_datasets])

        populate_tags_for_object_list(
            context['object_list'], model=self.object_list.model)

        if 'order' not in self.request.GET:
            context['order'] = 'source'
        else:
            context['order'] = self.request.GET.get('order')

        return context

    def get_paginate_by(self, queryset):
        """
        Paginate by specified value in querystring, or use default class property value.
        """
        return self.request.GET.get('paginate_by', self.paginate_by)

    def get_queryset(self):
        # get query terms from self.request
        qs = GlossRelation.objects.all()

        # Filter in only objects in the datasets the user has permissions to.
        allowed_datasets = get_objects_for_user(
            self.request.user, 'dictionary.view_dataset')
        qs = qs.filter(source__dataset__in=allowed_datasets).filter(
            target__dataset__in=allowed_datasets)

        get = self.request.GET

        # Search for multiple datasets (if provided)
        vals = get.getlist('dataset', [])
        if vals != []:
            qs = qs.filter(source__dataset__in=vals).filter(
                target__dataset__in=vals)

        if 'search' in get and get['search'] != '':
            val = get['search']
            # Searches for multiple fields at the same time. Looking if any of the fields match.
            query = (Q(source__idgloss__icontains=val) |
                     Q(target__idgloss__icontains=val))
            qs = qs.filter(query)

        if 'source' in get and get['source'] != '':
            val = get['source']
            # Search for sources glosses starting with a string, case sensitive
            query = Q(source__idgloss__istartswith=val)
            qs = qs.filter(query)

        if 'target' in get and get['target'] != '':
            val = get['target']
            # Search for sources glosses starting with a string, case sensitive
            query = Q(target__idgloss__istartswith=val)
            qs = qs.filter(query)

        if 'tags' in get and get['tags'] != '':
            vals = get.getlist('tags', [])
            tags = []
            for t in vals:
                tags.extend(Tag.objects.filter(pk=t))

            # search is an implicit AND so intersection
            tqs = TaggedItem.objects.get_intersection_by_model(GlossRelation, tags)

            # intersection
            qs = qs & tqs

        # Prefetching translation and dataset objects for glosses to minimize the amount of database queries.
        qs = qs.prefetch_related(Prefetch('source__dataset'), Prefetch('target__dataset'),
                                 Prefetch('source__glossvideo_set', queryset=GlossVideo.objects.all().order_by(
                                     'version')),
                                 Prefetch(
                                     'target__glossvideo_set', queryset=GlossVideo.objects.all().order_by('version'))
                                 )

        # Set order according to GET field 'order'
        if 'order' in get:
            qs = qs.order_by(get['order'])
        else:
            qs = qs.order_by('id')
        return qs
